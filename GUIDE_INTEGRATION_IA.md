# Guide d'int√©gration IA - Coro-Plus AI

## Guide technique pour int√©grer des mod√®les de Deep Learning

Ce document explique comment remplacer les algorithmes de traitement d'image actuels par de v√©ritables mod√®les d'Intelligence Artificielle bas√©s sur le Deep Learning.

---

## üìã Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Option 1 : Backend Python + API](#option-1--backend-python--api)
3. [Option 2 : TensorFlow.js (in-browser)](#option-2--tensorflowjs-in-browser)
4. [Pr√©paration des donn√©es](#pr√©paration-des-donn√©es)
5. [Entra√Ænement des mod√®les](#entra√Ænement-des-mod√®les)
6. [D√©ploiement](#d√©ploiement)

---

## Vue d'ensemble

Le syst√®me actuel utilise des algorithmes classiques (filtre bilat√©ral, seuillage) pour la d√©monstration. Pour une application clinique r√©elle, il faut int√©grer des mod√®les de Deep Learning.

### Architecture recommand√©e

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ
‚îÇ   (Next.js)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ Option 1: API REST ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                          ‚îÇ
         ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                    ‚îÇ  Backend  ‚îÇ
         ‚îÇ                    ‚îÇ  Python   ‚îÇ
         ‚îÇ                    ‚îÇ (FastAPI) ‚îÇ
         ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                          ‚îÇ
         ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                    ‚îÇ  Mod√®les  ‚îÇ
         ‚îÇ                    ‚îÇ  PyTorch  ‚îÇ
         ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ Option 2: TensorFlow.js
                    (in-browser)
```

---

## Option 1 : Backend Python + API

### Avantages
‚úÖ Performance optimale (GPU)
‚úÖ Flexibilit√© totale
‚úÖ Support PyTorch/TensorFlow complet
‚úÖ Pr√©traitement avanc√© possible

### Inconv√©nients
‚ùå Infrastructure serveur n√©cessaire
‚ùå Latence r√©seau
‚ùå Co√ªts d'h√©bergement

### √âtape 1 : Cr√©er le backend Python

#### Structure du projet

```
coro-plus-ai-backend/
‚îú‚îÄ‚îÄ app.py                    # FastAPI application
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ denoising.py         # Mod√®le de d√©bruitage
‚îÇ   ‚îî‚îÄ‚îÄ segmentation.py      # Mod√®le de segmentation
‚îú‚îÄ‚îÄ weights/
‚îÇ   ‚îú‚îÄ‚îÄ denoising_best.pth
‚îÇ   ‚îî‚îÄ‚îÄ segmentation_best.pth
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îî‚îÄ‚îÄ postprocessing.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

#### Code : app.py (FastAPI)

```python
from fastapi import FastAPI, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import torch
import numpy as np
from PIL import Image
import io
import base64

from models.denoising import DenoisingModel
from models.segmentation import SegmentationModel
from utils.preprocessing import preprocess_image
from utils.postprocessing import postprocess_image

app = FastAPI(title="Coro-Plus AI Backend")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Chargement des mod√®les
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
denoising_model = DenoisingModel().to(device)
denoising_model.load_state_dict(torch.load("weights/denoising_best.pth", map_location=device))
denoising_model.eval()

segmentation_model = SegmentationModel().to(device)
segmentation_model.load_state_dict(torch.load("weights/segmentation_best.pth", map_location=device))
segmentation_model.eval()

@app.post("/api/denoise")
async def denoise_image(file: UploadFile = File(...)):
    """
    Endpoint pour le d√©bruitage d'image
    """
    try:
        # Lire l'image
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert('L')
        
        # Pr√©traitement
        input_tensor = preprocess_image(image, device)
        
        # Inf√©rence
        with torch.no_grad():
            output_tensor = denoising_model(input_tensor)
        
        # Post-traitement
        output_image = postprocess_image(output_tensor)
        
        # Convertir en base64
        buffered = io.BytesIO()
        output_image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        # Calculer les m√©triques
        metrics = calculate_metrics(input_tensor, output_tensor)
        
        return JSONResponse({
            "success": True,
            "image": f"data:image/png;base64,{img_str}",
            "metrics": metrics
        })
        
    except Exception as e:
        return JSONResponse({
            "success": False,
            "error": str(e)
        }, status_code=500)

@app.post("/api/segment")
async def segment_image(file: UploadFile = File(...)):
    """
    Endpoint pour la segmentation
    """
    try:
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert('L')
        
        input_tensor = preprocess_image(image, device)
        
        with torch.no_grad():
            output_tensor = segmentation_model(input_tensor)
            output_tensor = torch.sigmoid(output_tensor)
        
        output_image = postprocess_segmentation(output_tensor)
        
        buffered = io.BytesIO()
        output_image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        return JSONResponse({
            "success": True,
            "image": f"data:image/png;base64,{img_str}"
        })
        
    except Exception as e:
        return JSONResponse({
            "success": False,
            "error": str(e)
        }, status_code=500)

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "device": str(device),
        "models_loaded": True
    }

def calculate_metrics(input_tensor, output_tensor):
    """
    Calcule les m√©triques de qualit√©
    """
    with torch.no_grad():
        # MSE
        mse = torch.mean((input_tensor - output_tensor) ** 2).item()
        
        # PSNR
        if mse > 0:
            psnr = 10 * np.log10(1.0 / mse)
        else:
            psnr = 100
        
        # SSIM (simplifi√©e)
        # Impl√©menter ou utiliser pytorch-msssim
        
    return {
        "mse": mse,
        "psnr": psnr,
        "processing_time_ms": 0  # √Ä mesurer
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)
```

#### Code : models/denoising.py

```python
import torch
import torch.nn as nn

class DenoisingAutoencoder(nn.Module):
    """
    Autoencodeur pour le d√©bruitage d'images m√©dicales
    Architecture inspir√©e de DnCNN et RED-Net
    """
    def __init__(self, in_channels=1, out_channels=1, features=64):
        super(DenoisingAutoencoder, self).__init__()
        
        # Encoder
        self.encoder = nn.Sequential(
            # Conv1
            nn.Conv2d(in_channels, features, kernel_size=3, padding=1),
            nn.BatchNorm2d(features),
            nn.ReLU(inplace=True),
            
            # Conv2
            nn.Conv2d(features, features, kernel_size=3, padding=1),
            nn.BatchNorm2d(features),
            nn.ReLU(inplace=True),
            
            # Conv3
            nn.Conv2d(features, features * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(features * 2),
            nn.ReLU(inplace=True),
        )
        
        # Middle layers
        self.middle = nn.Sequential(
            nn.Conv2d(features * 2, features * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(features * 2),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(features * 2, features * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(features * 2),
            nn.ReLU(inplace=True),
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            # Conv4
            nn.Conv2d(features * 2, features, kernel_size=3, padding=1),
            nn.BatchNorm2d(features),
            nn.ReLU(inplace=True),
            
            # Conv5
            nn.Conv2d(features, features, kernel_size=3, padding=1),
            nn.BatchNorm2d(features),
            nn.ReLU(inplace=True),
            
            # Output
            nn.Conv2d(features, out_channels, kernel_size=3, padding=1),
        )
        
    def forward(self, x):
        # Skip connection pour apprentissage r√©siduel
        identity = x
        
        # Encoder
        x = self.encoder(x)
        
        # Middle
        x = self.middle(x)
        
        # Decoder
        x = self.decoder(x)
        
        # Connexion r√©siduelle
        x = x + identity
        
        return x

class DenoisingModel(nn.Module):
    """
    Mod√®le de d√©bruitage avec skip connections
    """
    def __init__(self):
        super().__init__()
        self.model = DenoisingAutoencoder()
    
    def forward(self, x):
        return self.model(x)
```

#### Code : models/segmentation.py

```python
import torch
import torch.nn as nn

class UNet(nn.Module):
    """
    U-Net pour segmentation des art√®res coronaires
    """
    def __init__(self, in_channels=1, out_channels=1):
        super(UNet, self).__init__()
        
        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.pool1 = nn.MaxPool2d(2)
        
        self.enc2 = self.conv_block(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        
        self.enc3 = self.conv_block(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        
        self.enc4 = self.conv_block(256, 512)
        self.pool4 = nn.MaxPool2d(2)
        
        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)
        
        # Decoder
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)
        
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        # Output
        self.out = nn.Conv2d(64, out_channels, 1)
    
    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool1(enc1))
        enc3 = self.enc3(self.pool2(enc2))
        enc4 = self.enc4(self.pool3(enc3))
        
        # Bottleneck
        bottleneck = self.bottleneck(self.pool4(enc4))
        
        # Decoder avec skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.out(dec1)

class SegmentationModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = UNet()
    
    def forward(self, x):
        return self.model(x)
```

#### Code : utils/preprocessing.py

```python
import torch
import numpy as np
from PIL import Image

def preprocess_image(image: Image.Image, device: torch.device, target_size=512):
    """
    Pr√©traite l'image pour l'inf√©rence
    """
    # Redimensionner
    image = image.resize((target_size, target_size), Image.LANCZOS)
    
    # Convertir en array numpy
    img_array = np.array(image, dtype=np.float32)
    
    # Normaliser [0, 1]
    img_array = img_array / 255.0
    
    # Ajouter dimensions batch et channel
    if len(img_array.shape) == 2:
        img_array = img_array[np.newaxis, np.newaxis, :, :]
    
    # Convertir en tensor PyTorch
    tensor = torch.from_numpy(img_array).to(device)
    
    return tensor

def add_noise_for_training(image, noise_level=0.1):
    """
    Ajoute du bruit gaussien pour l'entra√Ænement
    """
    noise = torch.randn_like(image) * noise_level
    noisy_image = image + noise
    return torch.clamp(noisy_image, 0, 1)
```

#### Code : utils/postprocessing.py

```python
import torch
import numpy as np
from PIL import Image

def postprocess_image(tensor: torch.Tensor):
    """
    Convertit le tensor de sortie en image PIL
    """
    # D√©placer vers CPU et enlever dimensions batch/channel
    img_array = tensor.cpu().squeeze().numpy()
    
    # Clip entre 0 et 1
    img_array = np.clip(img_array, 0, 1)
    
    # Convertir en [0, 255]
    img_array = (img_array * 255).astype(np.uint8)
    
    # Cr√©er image PIL
    image = Image.fromarray(img_array, mode='L')
    
    return image

def postprocess_segmentation(tensor: torch.Tensor, threshold=0.5):
    """
    Post-traite la sortie de segmentation
    """
    # Appliquer seuil
    mask = (tensor > threshold).float()
    
    # Convertir en image
    img_array = mask.cpu().squeeze().numpy()
    img_array = (img_array * 255).astype(np.uint8)
    
    # Cr√©er image PIL en RGB pour visualisation
    image = Image.fromarray(img_array, mode='L')
    
    # Convertir en RGB et colorer en vert les zones segment√©es
    image_rgb = Image.new('RGB', image.size)
    pixels = image.load()
    pixels_rgb = image_rgb.load()
    
    for i in range(image.size[0]):
        for j in range(image.size[1]):
            if pixels[i, j] > 128:
                pixels_rgb[i, j] = (255, 0, 0)  # Rouge pour structures
            else:
                pixels_rgb[i, j] = (pixels[i, j], pixels[i, j], pixels[i, j])
    
    return image_rgb
```

#### Code : requirements.txt

```
fastapi==0.115.0
uvicorn[standard]==0.32.0
torch==2.5.0
torchvision==0.20.0
pillow==11.0.0
numpy==2.1.0
python-multipart==0.0.18
```

### √âtape 2 : Modifier le frontend Next.js

#### Code : lib/api.ts (nouveau fichier)

```typescript
export async function callDenoisingAPI(imageFile: File) {
  const formData = new FormData();
  formData.append('file', imageFile);
  
  const response = await fetch('http://localhost:5000/api/denoise', {
    method: 'POST',
    body: formData,
  });
  
  if (!response.ok) {
    throw new Error('API call failed');
  }
  
  const data = await response.json();
  return data;
}

export async function callSegmentationAPI(imageFile: File) {
  const formData = new FormData();
  formData.append('file', imageFile);
  
  const response = await fetch('http://localhost:5000/api/segment', {
    method: 'POST',
    body: formData,
  });
  
  if (!response.ok) {
    throw new Error('API call failed');
  }
  
  const data = await response.json();
  return data;
}
```

#### Modifier app/page.tsx

```typescript
// Remplacer dans handleProcess:
if (activeTab === 'denoising') {
  // Option avec API
  const apiResult = await callDenoisingAPI(selectedFile);
  if (apiResult.success) {
    setResult({
      originalImage: URL.createObjectURL(selectedFile),
      denoisedImage: apiResult.image,
      processingTime: apiResult.metrics.processing_time_ms,
      noiseReduction: apiResult.metrics.noise_reduction || 0,
      contrastImprovement: apiResult.metrics.contrast_improvement || 0,
    });
  }
}
```

---

## Option 2 : TensorFlow.js (in-browser)

### Avantages
‚úÖ Pas de backend n√©cessaire
‚úÖ Latence minimale
‚úÖ Pas de co√ªts serveur
‚úÖ Privacy (donn√©es restent locales)

### Inconv√©nients
‚ùå Performance limit√©e (CPU/WebGL)
‚ùå Taille des mod√®les limit√©e
‚ùå Moins flexible

### √âtape 1 : Entra√Æner et convertir le mod√®le

```python
# train_and_export.py
import tensorflow as tf
import tensorflowjs as tfjs

# Apr√®s entra√Ænement du mod√®le
model.save('denoising_model')

# Convertir en TensorFlow.js
tfjs.converters.save_keras_model(model, 'public/models/denoising')
```

### √âtape 2 : Utiliser dans Next.js

```bash
npm install @tensorflow/tfjs
```

```typescript
// lib/tfjs-inference.ts
import * as tf from '@tensorflow/tfjs';

let denoisingModel: tf.LayersModel | null = null;

export async function loadDenoisingModel() {
  if (!denoisingModel) {
    denoisingModel = await tf.loadLayersModel('/models/denoising/model.json');
  }
  return denoisingModel;
}

export async function denoise TensorFlowJS(imageFile: File): Promise<string> {
  const model = await loadDenoisingModel();
  
  // Charger l'image
  const img = new Image();
  img.src = URL.createObjectURL(imageFile);
  await img.decode();
  
  // Convertir en tensor
  let tensor = tf.browser.fromPixels(img, 1);
  tensor = tensor.div(255.0);
  tensor = tensor.expandDims(0);
  
  // Inf√©rence
  const output = model.predict(tensor) as tf.Tensor;
  
  // Post-traitement
  const outputData = await output.mul(255).squeeze().array() as number[][];
  
  // Cr√©er image
  const canvas = document.createElement('canvas');
  canvas.width = img.width;
  canvas.height = img.height;
  const ctx = canvas.getContext('2d')!;
  const imageData = ctx.createImageData(img.width, img.height);
  
  for (let i = 0; i < img.height; i++) {
    for (let j = 0; j < img.width; j++) {
      const idx = (i * img.width + j) * 4;
      const value = outputData[i][j];
      imageData.data[idx] = value;
      imageData.data[idx + 1] = value;
      imageData.data[idx + 2] = value;
      imageData.data[idx + 3] = 255;
    }
  }
  
  ctx.putImageData(imageData, 0, 0);
  return canvas.toDataURL('image/png');
}
```

---

## Pr√©paration des donn√©es

### Dataset recommand√©s

1. **ASOCA (Automated Segmentation of Coronary Arteries)**
2. **CA-500**
3. **Donn√©es simul√©es avec bruit ajout√©**

### Script de pr√©paration

```python
# prepare_dataset.py
import os
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset

class CoronaryDataset(Dataset):
    def __init__(self, root_dir, noise_level=0.1, transform=None):
        self.root_dir = root_dir
        self.noise_level = noise_level
        self.transform = transform
        self.images = os.listdir(root_dir)
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.images[idx])
        image = Image.open(img_name).convert('L')
        
        if self.transform:
            image = self.transform(image)
        
        # Image propre (target)
        clean = np.array(image, dtype=np.float32) / 255.0
        
        # Image bruit√©e (input)
        noise = np.random.normal(0, self.noise_level, clean.shape)
        noisy = np.clip(clean + noise, 0, 1)
        
        clean_tensor = torch.from_numpy(clean).unsqueeze(0)
        noisy_tensor = torch.from_numpy(noisy.astype(np.float32)).unsqueeze(0)
        
        return noisy_tensor, clean_tensor
```

---

## Entra√Ænement des mod√®les

### Script d'entra√Ænement

```python
# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from models.denoising import DenoisingModel
from prepare_dataset import CoronaryDataset

def train_denoising_model():
    # Param√®tres
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_epochs = 100
    batch_size = 16
    learning_rate = 0.001
    
    # Dataset
    dataset = CoronaryDataset('data/train', noise_level=0.1)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # Mod√®le
    model = DenoisingModel().to(device)
    
    # Loss et optimizer
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # Training loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        for batch_idx, (noisy, clean) in enumerate(dataloader):
            noisy = noisy.to(device)
            clean = clean.to(device)
            
            # Forward
            output = model(noisy)
            loss = criterion(output, clean)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')
        
        # Sauvegarder
        if (epoch + 1) % 10 == 0:
            torch.save(model.state_dict(), f'weights/denoising_epoch_{epoch+1}.pth')
    
    # Sauvegarder le mod√®le final
    torch.save(model.state_dict(), 'weights/denoising_best.pth')
    print('Training complete!')

if __name__ == '__main__':
    train_denoising_model()
```

---

## D√©ploiement

### Option A : D√©ploiement local

```bash
# Terminal 1 : Backend Python
cd coro-plus-ai-backend
pip install -r requirements.txt
python app.py

# Terminal 2 : Frontend Next.js
cd coro-plus-ai
npm run dev
```

### Option B : D√©ploiement cloud

#### Backend sur Render/Railway

```yaml
# render.yaml
services:
  - type: web
    name: coro-plus-ai-api
    runtime: python
    buildCommand: pip install -r requirements.txt
    startCommand: python app.py
```

#### Frontend sur Vercel

```bash
npm run build
vercel deploy
```

---

## Conclusion

Ce guide fournit deux approches compl√®tes pour int√©grer de l'IA r√©elle dans Coro-Plus AI. Choisissez l'option qui correspond le mieux √† vos contraintes de temps, budget et infrastructure.

Pour un prototype acad√©mique rapide ‚Üí Option 2 (TensorFlow.js)
Pour une application performante ‚Üí Option 1 (Backend Python)
